{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e89dad-9642-4260-a6ec-34d25029dd08",
   "metadata": {},
   "source": [
    "# Minimum Variance Portfolio Optimization with WRDS (CRSP Monthly Data)\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Pull monthly stock return data from WRDS/CRSP\n",
    "- Clean and validate coverage\n",
    "- Estimate covariance matrices using **sample** and **Ledoit-Wolf shrinkage**\n",
    "- Solve the **Global Minimum Variance (GMV)** portfolio with constraints\n",
    "- Run a **rolling out-of-sample backtest**\n",
    "- Compute metrics: annualized return, volatility, Sharpe ratio\n",
    "- Analyze **turnover** and **weight stability**\n",
    "- Export results and plots for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037d8c02-b6ef-4c5f-9ddd-86a6cb0a758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca0d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import wrds\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def data_acquisition():\n",
    "\n",
    "    # Connect to WRDS database\n",
    "    db = db_connect()\n",
    "\n",
    "    # Get user inputs\n",
    "    tickers = ticker_input()\n",
    "    start_date_eval, look_back_period = date_input()\n",
    "    max_missing = missing_months_input()\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    shrcd_list = [10, 11]  # Common shares\n",
    "\n",
    "    # Get valid PERMNOs for the tickers\n",
    "    permno_list, valid_tickers = get_active_permnos(\n",
    "        db, tickers, start_date_eval, max_missing, shrcd_list\n",
    "    )\n",
    "\n",
    "    # Get returns data\n",
    "    all_data = get_returns(db, permno_list, start_date_eval.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # Make the PERMNO columns into ticker columns and rename\n",
    "    permno_to_ticker = dict(zip(permno_list, valid_tickers))\n",
    "    all_data = all_data.rename(columns=permno_to_ticker)\n",
    "\n",
    "    # Ask the user if we should set weight constraints\n",
    "    max_weight, min_weight = weight_constraint_input()\n",
    "\n",
    "    # Get the end date for the full data set\n",
    "    end_date_eval = all_data.index.max()\n",
    "    # Get the risk-free rate series based on the date range of the returns data\n",
    "    risk_free_rate_series = get_risk_free_rate_series(\n",
    "        db, start_date_eval, end_date_eval\n",
    "    )\n",
    "\n",
    "    df_full = get_crsp_monthly_panel(\n",
    "        db, permno_list, start_date_eval.strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "\n",
    "    # Close the WRDS connection\n",
    "    db.close()\n",
    "\n",
    "    return (\n",
    "        all_data,\n",
    "        valid_tickers,\n",
    "        permno_list,\n",
    "        start_date_eval,\n",
    "        look_back_period,\n",
    "        max_weight,\n",
    "        min_weight,\n",
    "        risk_free_rate_series,\n",
    "        df_full,\n",
    "    )\n",
    "\n",
    "\n",
    "def db_connect():\n",
    "    # Connect to WRDS database\n",
    "    try:\n",
    "        db = wrds.Connection()\n",
    "        print(\"Connected to WRDS database.\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(\"Failed to connect to WRDS database:\", e)\n",
    "        raise\n",
    "\n",
    "\n",
    "def ticker_input():\n",
    "    # Ask the user for a list of tickers (comma separated) with error handling\n",
    "    while True:\n",
    "        # Split by comma and strip whitespace and remove \"\n",
    "        tickers = input(\"Enter a list of stock tickers (comma separated): \").split(\",\")\n",
    "        tickers = [ticker.strip().upper() for ticker in tickers]\n",
    "        # Remove any surrounding quotes from each ticker\n",
    "        tickers = [ticker.replace('\"', \"\").replace(\"'\", \"\") for ticker in tickers]\n",
    "        # If the list is empty or contains only empty strings, ask again\n",
    "        if not tickers or tickers == [\"\"]:\n",
    "            print(\"Invalid input. Please enter at least one ticker.\")\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Tickers: {tickers}\")\n",
    "    return tickers\n",
    "\n",
    "\n",
    "def date_input():\n",
    "    \"\"\"\n",
    "    Prompt for a start date (YYYY-MM-DD) and number of months (positive int),\n",
    "    then return (start_date, number of months).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input for start date and look back period\n",
    "            start_str = input(\"Enter the start date (YYYY-MM-DD): \").strip()\n",
    "            months_str = input(\n",
    "                \"Enter the number of months for the look back period: \"\n",
    "            ).strip()\n",
    "\n",
    "            # Strict date parsing (won't accept \"0\")\n",
    "            start_date = dt.strptime(start_str, \"%Y-%m-%d\")\n",
    "            num_months = int(months_str)\n",
    "\n",
    "            # If the number of months is not positive, ask again\n",
    "            if num_months <= 0:\n",
    "                print(\"Number of months must be a positive integer. Please try again.\")\n",
    "                continue\n",
    "\n",
    "            # Compute and display the end date\n",
    "            print(\n",
    "                f\"Date range: {pd.Timestamp(start_date).date()}, Number of months: {num_months}\"\n",
    "            )\n",
    "            return pd.Timestamp(start_date), num_months\n",
    "\n",
    "        except ValueError:\n",
    "            print(\n",
    "                \"Invalid input. Use YYYY-MM-DD for the date and a positive integer for months.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def missing_months_input():\n",
    "    # Ask the user for the maximum number of allowed missing months with error handling\n",
    "    while True:\n",
    "        try:\n",
    "            # Ask the user for the maximum number of allowed missing months with error handling\n",
    "            max_missing = int(\n",
    "                input(\"Enter the maximum number of allowed missing months: \")\n",
    "            )\n",
    "            # If the number is negative, ask again\n",
    "            if max_missing < 0:\n",
    "                print(\"Please enter a non-negative integer.\")\n",
    "            else:\n",
    "                break\n",
    "        # If the input is not an integer, ask again\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a non-negative integer.\")\n",
    "    print(f\"Maximum allowed missing months: {max_missing}\")\n",
    "    return max_missing\n",
    "\n",
    "\n",
    "def get_active_permnos(\n",
    "    db, tickers: list, start_date: pd.Timestamp, max_missing: int, shrcd_list: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Get active PERMNOs for the given tickers, max_missing, shrcd_list, and date range.\n",
    "    \"\"\"\n",
    "    permno_list = []\n",
    "    valid_tickers = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Get the active PERMNO for the ticker\n",
    "        permno = pick_permno_for_ticker(db, ticker, start_date, max_missing, shrcd_list)\n",
    "        # If there is no valid PERMNO found, ask the user for a replacement ticker\n",
    "        while permno is None:\n",
    "            print(f\"No valid PERMNO found for ticker {ticker}.\")\n",
    "            ticker = input(\"Please enter a replacement ticker: \").strip().upper()\n",
    "            permno = pick_permno_for_ticker(\n",
    "                db, ticker, start_date, max_missing, shrcd_list\n",
    "            )\n",
    "        # If a valid PERMNO is found, add the PERMNO and ticker to the list\n",
    "        permno_list.append(permno)\n",
    "        valid_tickers.append(ticker)\n",
    "\n",
    "    # Print the final list of valid tickers and PERMNOs\n",
    "    print(f\"Final tickers used: {valid_tickers}\")\n",
    "    print(f\"Corresponding PERMNOs: {permno_list}\")\n",
    "\n",
    "    return permno_list, valid_tickers\n",
    "\n",
    "\n",
    "def pick_permno_for_ticker(\n",
    "    db, ticker: str, start_date: pd.Timestamp, max_missing: int, shrcd_list: list\n",
    ") -> int | None:\n",
    "    \"\"\"\n",
    "    Given a ticker and a date, pick the most appropriate PERMNO.\n",
    "\n",
    "    Criteria (in order):\n",
    "    1) Active on the start_date\n",
    "    2) If multiple, pick the one with the namedt earliest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Selected PERMNO or None if no valid PERMNO found. Delisting dates.\n",
    "    \"\"\"\n",
    "    q = f\"\"\"\n",
    "    SELECT\n",
    "        n.permno,\n",
    "        n.namedt,\n",
    "        n.nameendt,\n",
    "        n.exchcd,\n",
    "        n.shrcd\n",
    "    FROM crsp.msenames AS n\n",
    "    WHERE n.ticker = '{ticker}'\n",
    "    ORDER BY n.namedt;\n",
    "    \"\"\"\n",
    "    df = db.raw_sql(q)\n",
    "\n",
    "    df[\"namedt\"] = pd.to_datetime(df[\"namedt\"])\n",
    "\n",
    "    # IF there are no matches for the ticker, return None\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # Filter to only common shares in the allowed share codes\n",
    "    df = df[df[\"shrcd\"].isin(shrcd_list)]\n",
    "\n",
    "    if df.empty:\n",
    "        # Nothing in the allowed share codes\n",
    "        print(\n",
    "            f\"Ticker {ticker} has no active PERMNOs in common share classes as of {start_date.date()}.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Merge overlapping or touching name ranges per PERMNO\n",
    "    df = collapse_name_ranges(df)\n",
    "\n",
    "    # Filter to only PERMNOs active on the start_date\n",
    "    active = df[df[\"namedt\"] <= start_date]\n",
    "    if active.empty:\n",
    "        print(f\"Ticker {ticker} has no active PERMNOs as of {start_date.date()}.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate over active PERMNOs and check for delisting and returns data\n",
    "    for _, row in active.iterrows():\n",
    "        permno = int(row[\"permno\"])\n",
    "\n",
    "        df1 = get_delistings(db, [permno], start_date.strftime(\"%Y-%m-%d\"))\n",
    "        df2 = get_returns(db, [permno], start_date.strftime(\"%Y-%m-%d\"), max_missing)\n",
    "\n",
    "        # If no delisting data and if the return data has < max_missing months, pick this PERMNO\n",
    "        if df1.empty and isinstance(df2, pd.DataFrame) and not df2.empty:\n",
    "            return permno\n",
    "        elif not df1.empty:\n",
    "            print(\"Delisting data found for PERMNO:\", permno)\n",
    "            print(df1)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def collapse_name_ranges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge rows per permno when the next segment starts the day after (or earlier than)\n",
    "    the previous segment ends (i.e., overlapping or touching intervals).\n",
    "\n",
    "    Keeps only: permno, namedt, nameendt\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    out = df.copy()\n",
    "\n",
    "    # Ensure date columns are Timestamps and handle open-ended intervals\n",
    "    out[\"namedt\"] = pd.to_datetime(out[\"namedt\"])\n",
    "    out[\"nameendt\"] = pd.to_datetime(out[\"nameendt\"]).fillna(pd.Timestamp(\"9999-12-31\"))\n",
    "\n",
    "    # Sort within permno by start then end date\n",
    "    out = out.sort_values([\"permno\", \"namedt\", \"nameendt\"])\n",
    "\n",
    "    # Group by permno and shift to get previous end date\n",
    "    prev_end = out.groupby(\"permno\")[\"nameendt\"].shift()\n",
    "\n",
    "    # Start a new group when there is a gap (> 1 day).\n",
    "    is_new_group = (prev_end.isna()) | (\n",
    "        out[\"namedt\"] > (prev_end + pd.Timedelta(days=1))\n",
    "    )\n",
    "\n",
    "    # Running group id per permno\n",
    "    out[\"grp\"] = is_new_group.groupby(out[\"permno\"]).cumsum()\n",
    "\n",
    "    # Aggregate each group to a single interval\n",
    "    merged = (\n",
    "        out.groupby([\"permno\", \"grp\"])\n",
    "        .agg(namedt=(\"namedt\", \"min\"), nameendt=(\"nameendt\", \"max\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return merged[[\"permno\", \"namedt\", \"nameendt\"]]\n",
    "\n",
    "\n",
    "def get_returns(\n",
    "    df_conn, permno_list: list, start: str, max_missing=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wide monthly returns: index=date (Timestamp), columns=permno (int), values=ret (float).\n",
    "    Missing/non-numeric RET are filled with 0.0 (policy choice).\n",
    "    \"\"\"\n",
    "    # If the permno_list is empty, return an empty DataFrame\n",
    "    if not permno_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert the values in the permno_list to integers\n",
    "    permno_list = [int(p) for p in permno_list]\n",
    "\n",
    "    q = f\"\"\"\n",
    "        SELECT permno, date, ret\n",
    "        FROM crsp.msf\n",
    "        WHERE permno IN ({','.join(map(str, permno_list))})\n",
    "          AND date >= '{start}'\n",
    "        ORDER BY permno, date;\n",
    "    \"\"\"\n",
    "    d = df_conn.raw_sql(q)\n",
    "\n",
    "    # If the query returned no data, return an empty DataFrame\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(index=pd.DatetimeIndex([], name=\"date\"))\n",
    "\n",
    "    # Ensure correct dtypes and alignment to month-end\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"]) + MonthEnd(0)  # ensure month-end\n",
    "    d[\"ret\"] = pd.to_numeric(d[\"ret\"], errors=\"coerce\")\n",
    "\n",
    "    # Count missing per PERMNO before filling the output data\n",
    "    if d[\"ret\"].isna().any():\n",
    "        miss_counts = d.loc[d[\"ret\"].isna()].groupby(\"permno\")[\"ret\"].size()\n",
    "    else:\n",
    "        # If no missing, create a zero Series for all PERMNOs\n",
    "        miss_counts = pd.Series(0, index=d[\"permno\"].unique(), name=\"ret\")\n",
    "\n",
    "    # Enforce per-PERMNO missing threshold (if requested)\n",
    "    if max_missing is not None:\n",
    "        # Identify bad PERMNOs based on missing counts and max_missing\n",
    "        bad_permnos = miss_counts[miss_counts > max_missing]\n",
    "        # If any bad PERMNOs are found, print a warning and drop them\n",
    "        if not bad_permnos.empty:\n",
    "            print(\n",
    "                f\"Warning: dropping PERMNOs exceeding allowed missing months ({max_missing}): \"\n",
    "                f\"{sorted(map(int, bad_permnos.index.tolist()))}\"\n",
    "            )\n",
    "            d = d[~d[\"permno\"].isin(bad_permnos.index)]\n",
    "\n",
    "    # If everything was dropped, return an empty, well-formed frame\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(index=pd.DatetimeIndex([], name=\"date\"))\n",
    "\n",
    "    # Fill remaining missing to 0.0\n",
    "    n_missing_remaining = d[\"ret\"].isna().sum()\n",
    "    if n_missing_remaining:\n",
    "        print(\n",
    "            f\"Info: {n_missing_remaining} missing returns (after filtering) \"\n",
    "            f\"coerced to 0.0 across {d['permno'].nunique()} PERMNO(s).\"\n",
    "        )\n",
    "        d[\"ret\"] = d[\"ret\"].fillna(0.0)\n",
    "\n",
    "    # Pivot to wide format\n",
    "    wide = (\n",
    "        d.pivot(index=\"date\", columns=\"permno\", values=\"ret\")\n",
    "        .sort_index()\n",
    "        .rename_axis(index=\"date\", columns=\"permno\")\n",
    "        .astype(float)\n",
    "    )\n",
    "    return wide\n",
    "\n",
    "\n",
    "def get_delistings(db, permno_list: list, start: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Delisting returns aligned to month-end for merging with monthly CRSP returns.\n",
    "    Returns long DataFrame with columns: permno (int), date (Timestamp month-end), dlret (float).\n",
    "    \"\"\"\n",
    "    # If the permno_list is empty, return an empty DataFrame\n",
    "    if not permno_list:\n",
    "        return pd.DataFrame(columns=[\"permno\", \"date\", \"dlret\"])\n",
    "\n",
    "    # Convert the values in the permno_list to integers\n",
    "    permno_list = [int(p) for p in permno_list]\n",
    "\n",
    "    q = f\"\"\"\n",
    "        SELECT permno, dlstdt, dlret\n",
    "        FROM crsp.msedelist\n",
    "        WHERE permno IN ({','.join(map(str, permno_list))})\n",
    "          AND dlstdt >= '{start}'\n",
    "          AND dlret IS NOT NULL\n",
    "        ORDER BY permno, dlstdt;\n",
    "    \"\"\"\n",
    "    df = db.raw_sql(q)\n",
    "\n",
    "    # If the query returned no data, return an empty DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"permno\", \"date\", \"dlret\"])\n",
    "\n",
    "    # Ensure correct dtypes and alignment to month-end\n",
    "    df[\"date\"] = pd.to_datetime(df[\"dlstdt\"]) + MonthEnd(0)  # align to month-end\n",
    "    df[\"dlret\"] = pd.to_numeric(df[\"dlret\"], errors=\"coerce\")\n",
    "    df = df[[\"permno\", \"date\", \"dlret\"]].dropna(subset=[\"dlret\"])\n",
    "    df[\"permno\"] = df[\"permno\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_effective_returns(\n",
    "    panel_wide: pd.DataFrame, delist_long: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge delisting returns into a wide monthly returns panel and compute effective returns:\n",
    "        ret_eff = (1 + ret) * (1 + dlret) - 1\n",
    "    Returns a wide DataFrame with the same shape (date x permno), values=ret_eff.\n",
    "    - If a month has both ret and dlret, they compound.\n",
    "    - If only one is present, the other is treated as 0 (no effect).\n",
    "    \"\"\"\n",
    "    if panel_wide.empty:\n",
    "        return panel_wide\n",
    "    if delist_long.empty:\n",
    "        return panel_wide\n",
    "\n",
    "    panel_wide = panel_wide.rename_axis(index=\"date\", columns=\"permno\")\n",
    "\n",
    "    # Wide -> long for merge\n",
    "    long = (\n",
    "        panel_wide.stack()\n",
    "        .rename(\"ret\")\n",
    "        .reset_index()\n",
    "        # not needed if columns already named 'permno'\n",
    "        .rename(columns={\"level_1\": \"permno\"})\n",
    "    )\n",
    "    # Ensure correct dtypes\n",
    "    long[\"permno\"] = long[\"permno\"].astype(int)\n",
    "    long[\"date\"] = pd.to_datetime(long[\"date\"])\n",
    "\n",
    "    # Merge and compute effective returns\n",
    "    merged = long.merge(delist_long, on=[\"permno\", \"date\"], how=\"left\")\n",
    "    merged[\"ret\"] = pd.to_numeric(merged[\"ret\"], errors=\"coerce\").fillna(0.0)\n",
    "    merged[\"dlret\"] = pd.to_numeric(merged[\"dlret\"], errors=\"coerce\").fillna(0.0)\n",
    "    merged[\"ret_eff\"] = (1.0 + merged[\"ret\"]) * (1.0 + merged[\"dlret\"]) - 1.0\n",
    "\n",
    "    # Long -> wide\n",
    "    out = (\n",
    "        merged.pivot(index=\"date\", columns=\"permno\", values=\"ret_eff\")\n",
    "        .sort_index()\n",
    "        .rename_axis(index=\"date\", columns=\"permno\")\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    # Preserve original index/columns union (optional)\n",
    "    out = out.reindex(index=panel_wide.index.union(out.index)).reindex(\n",
    "        columns=panel_wide.columns, fill_value=out.reindex(columns=panel_wide.columns)\n",
    "    )\n",
    "    # align to original dates/permnos\n",
    "    out = out.reindex(index=panel_wide.index, columns=panel_wide.columns)\n",
    "    return out\n",
    "\n",
    "\n",
    "def weight_constraint_input():\n",
    "    # Ask the user if they want to set max/min weight constraints\n",
    "    def ask_yn(prompt: str) -> str:\n",
    "        while True:\n",
    "            # Ask the user for a 'y' or 'n' answer\n",
    "            ans = input(prompt).strip().lower()\n",
    "            if ans in {\"y\", \"n\"}:\n",
    "                return ans\n",
    "            print(\"Please answer 'y' or 'n'.\")\n",
    "\n",
    "    # ---- max (long) constraint ----\n",
    "    set_max = ask_yn(\"Do you want to set maximum weight constraints? (y/n): \")\n",
    "    max_weight = None\n",
    "    # If yes, ask for the max weight value with error handling\n",
    "    if set_max == \"y\":\n",
    "        while True:\n",
    "            try:\n",
    "                # Ask the user for the maximum weight value with error handling\n",
    "                max_weight = float(\n",
    "                    input(\n",
    "                        \"Enter the maximum long position (e.g., 0.1 for 10%): \"\n",
    "                    ).strip()\n",
    "                )\n",
    "                # If the number is not positive, ask again\n",
    "                if max_weight <= 0:\n",
    "                    print(\"Please enter a positive number.\")\n",
    "                    continue\n",
    "                break\n",
    "            # If the input is not a number, ask again\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a positive number.\")\n",
    "        print(f\"Maximum long position: {max_weight}\")\n",
    "    else:\n",
    "        print(\"No maximum weight constraints set.\")\n",
    "\n",
    "    # ---- min (short) constraint ----\n",
    "    set_min = ask_yn(\"Do you want to set maximum short position constraints? (y/n): \")\n",
    "    min_weight = None\n",
    "    # If yes, ask for the min weight value with error handling\n",
    "    if set_min == \"y\":\n",
    "        while True:\n",
    "            try:\n",
    "                min_weight = float(\n",
    "                    input(\n",
    "                        \"Enter the maximum short position (e.g., -0.1 for -10%): \"\n",
    "                    ).strip()\n",
    "                )\n",
    "                if min_weight > 0:\n",
    "                    print(\"Please enter a negative number (or zero for no shorting).\")\n",
    "                    continue\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a negative number.\")\n",
    "        print(f\"Maximum short position: {min_weight}\")\n",
    "    else:\n",
    "        print(\"No minimum weight constraints set.\")\n",
    "\n",
    "    return max_weight, min_weight\n",
    "\n",
    "\n",
    "def window_size_input():\n",
    "    # Ask the user for the rolling window size (in months) with error handling\n",
    "    while True:\n",
    "        try:\n",
    "            window_size = int(\n",
    "                input(\n",
    "                    \"Enter the rolling window size (in months) for out-of-sample testing: \"\n",
    "                )\n",
    "            )\n",
    "            if window_size <= 0:\n",
    "                print(\"Please enter a positive integer.\")\n",
    "            else:\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a positive integer.\")\n",
    "    print(f\"Rolling window size: {window_size} months\")\n",
    "    return window_size\n",
    "\n",
    "\n",
    "def risk_free_rate_input():\n",
    "    # Ask the user for the annual risk-free rate (as a decimal) with error handling\n",
    "    while True:\n",
    "        try:\n",
    "            rf_rate = float(\n",
    "                input(\n",
    "                    \"Enter the annual risk-free rate (as a decimal, e.g., 0.03 for 3%): \"\n",
    "                )\n",
    "            )\n",
    "            if rf_rate < 0:\n",
    "                print(\"Please enter a non-negative number.\")\n",
    "            else:\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a non-negative number.\")\n",
    "    print(f\"Annual risk-free rate: {rf_rate}\")\n",
    "    return rf_rate\n",
    "\n",
    "\n",
    "def get_risk_free_rate_series(db, start, end) -> pd.DataFrame:\n",
    "    rf = db.get_table(library=\"ff\", table=\"factors_monthly\")[[\"date\", \"rf\"]]\n",
    "    rf[\"date\"] = pd.to_datetime(rf[\"date\"]) + MonthEnd(0)  # ensure month-end\n",
    "    rf = rf.rename(columns={\"rf\": \"risk_free_rate\"})\n",
    "    rf = rf[(rf[\"date\"] >= start) & (rf[\"date\"] <= end)]\n",
    "    rf.set_index(\"date\", inplace=True)\n",
    "    return rf\n",
    "\n",
    "\n",
    "def get_crsp_monthly_panel(db, permono_list: list, start_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull the CRSP monthly panel *with* security descriptors via a time-bounded\n",
    "    join to msenames (since shrcd/exchcd live in the name-history tables).\n",
    "\n",
    "    Filters (can be edited):\n",
    "      - U.S. common stocks: shrcd in (10, 11)\n",
    "      - Major exchanges   : exchcd in (1, 2, 3)  # NYSE, AMEX, NASDAQ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with date, permno, prc, shrout, ret, retx, shrcd, exchcd,\n",
    "    and a computed market cap (mktcap).\n",
    "    \"\"\"\n",
    "    # Convert start/end to Timestamps\n",
    "    # start_dt = pd.to_datetime(start_date)\n",
    "\n",
    "    q = f\"\"\"\n",
    "    SELECT\n",
    "        m.date,\n",
    "        m.permno,\n",
    "        m.permco,\n",
    "        m.prc,\n",
    "        m.shrout,\n",
    "        m.ret,\n",
    "        m.retx,\n",
    "        n.shrcd,\n",
    "        n.exchcd,\n",
    "        n.ticker\n",
    "    FROM crsp.msf AS m\n",
    "    JOIN crsp.msenames AS n\n",
    "        ON m.permno = n.permno\n",
    "        AND m.date BETWEEN n.namedt AND COALESCE(n.nameendt, '9999-12-31') \n",
    "        -- A given stock (permno) can have multiple records in msenames over time: ticker changes, exchange moves, share code reclassifications, mergers.\n",
    "    WHERE m.date >= '{start_date}'\n",
    "        AND m.permno IN ({','.join(map(str, permono_list))})\n",
    "        AND m.prc    IS NOT NULL\n",
    "        AND m.shrout IS NOT NULL\n",
    "    ORDER BY m.date, m.permno;\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate all yearly chunks\n",
    "    df = db.raw_sql(q)\n",
    "\n",
    "    # ---- Standardize dtypes and construct market cap ----\n",
    "    if not df.empty:\n",
    "        # Ensure correct dtypes\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]) + MonthEnd(0)  # ensure month-end\n",
    "        # CRSP price may be negative (indicating a bid/ask mid)\n",
    "        df[\"prc\"] = pd.to_numeric(df[\"prc\"], errors=\"coerce\").abs()\n",
    "        # CRSP shares outstanding is in thousands\n",
    "        df[\"shrout\"] = (\n",
    "            pd.to_numeric(df[\"shrout\"], errors=\"coerce\") * 1000.0\n",
    "        )  # in thousands of shares\n",
    "        df[\"ret\"] = pd.to_numeric(df.get(\"ret\", pd.Series()), errors=\"coerce\")\n",
    "        df[\"retx\"] = pd.to_numeric(df.get(\"retx\", pd.Series()), errors=\"coerce\")\n",
    "        # Calculate market cap based on price and shares outstanding\n",
    "        df[\"mktcap\"] = df[\"prc\"] * df[\"shrout\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df6a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def returns_to_excess(\n",
    "    returns: pd.DataFrame, rf_df: pd.DataFrame | pd.Series\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert raw returns to excess returns by subtracting risk-free rate.\n",
    "    returns : DataFrame of raw returns (assets in columns)\n",
    "    \"\"\"\n",
    "    # Deep copy to avoid modifying input\n",
    "    out = deepcopy(returns)\n",
    "    # If rf_df is a 1-col DataFrame, squeeze to a Series\n",
    "    rf = rf_df.squeeze() if isinstance(rf_df, pd.DataFrame) else rf_df\n",
    "    # Align by index (in case one has extra/missing dates)\n",
    "    rf = rf.reindex(out.index)\n",
    "    # Vectorized row-wise subtraction across all columns\n",
    "    out = out.sub(rf, axis=0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_covariance(ret_window):\n",
    "    \"\"\"\n",
    "    Calculate sample covariance matrix from return window.\n",
    "    \"\"\"\n",
    "    return ret_window.cov().values\n",
    "\n",
    "\n",
    "def ledoit_wolf_covariance(ret_window):\n",
    "    \"\"\"\n",
    "    Calculate Ledoit-Wolf shrinkage covariance matrix from return window.\n",
    "    \"\"\"\n",
    "    lw = LedoitWolf().fit(ret_window.values)\n",
    "    return lw.covariance_\n",
    "\n",
    "\n",
    "def gmv_weights(cov_matrix, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Calculate Global Minimum Variance portfolio weights given a covariance matrix.\n",
    "    lower_bound : minimum weight constraint (None for no constraint)\n",
    "    upper_bound : maximum weight constraint (None for no constraint)\n",
    "    \"\"\"\n",
    "    # Number of assets\n",
    "    n = cov_matrix.shape[0]\n",
    "    # Define optimization variable\n",
    "    w = cp.Variable(n)\n",
    "\n",
    "    # Define objective and constraints\n",
    "    objective = cp.Minimize(cp.quad_form(w, cov_matrix))\n",
    "\n",
    "    # Add sum-to-one constraint\n",
    "    constraints = [cp.sum(w) == 1]\n",
    "    # Add maximum weight constraints if specified\n",
    "    if upper_bound is not None:\n",
    "        constraints.append(w <= upper_bound)\n",
    "    # Add minimum weight constraints if specified\n",
    "    if lower_bound is not None:\n",
    "        constraints.append(w >= lower_bound)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.OSQP, verbose=False)\n",
    "\n",
    "    # Return the optimized weights as a 1D numpy array\n",
    "    return np.array(w.value).ravel()\n",
    "\n",
    "\n",
    "def backtest_minvar(\n",
    "    raw_returns_wide,\n",
    "    excess_returns_wide,\n",
    "    window_months=36,\n",
    "    lower_bound=None,\n",
    "    upper_bound=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate rolling out-of-sample performance of GMV portfolios\n",
    "    raw_returns_wide    : DataFrame of raw returns (assets in columns)\n",
    "    excess_returns_wide : DataFrame of excess returns (assets in columns)\n",
    "    window_months       : look-back window size in months\n",
    "    lower_bound         : minimum weight constraint (None for no constraint)\n",
    "    upper_bound         : maximum weight constraint (None for no constraint)\n",
    "\n",
    "    Returns\n",
    "    perf_df   : DataFrame with return and cumulative return columns\n",
    "    weights_df: DataFrame with weight history in long format\n",
    "    \"\"\"\n",
    "    # Get dates and tickers\n",
    "    dates = excess_returns_wide.index\n",
    "    tickers = list(excess_returns_wide.columns)\n",
    "    # Initialize lists to store performance and weights\n",
    "    perf_rows, weight_rows = [], []\n",
    "\n",
    "    # Loop over each month in the backtest period\n",
    "    for t in range(window_months, len(dates) - 1):\n",
    "        # The estimation window is from t-window to t\n",
    "        est_window = excess_returns_wide.iloc[t - window_months : t]\n",
    "\n",
    "        # The next month's returns to evaluate performance\n",
    "        next_raw_ret = raw_returns_wide.iloc[t + 1].values\n",
    "        next_excess_ret = excess_returns_wide.iloc[t + 1].values\n",
    "\n",
    "        # Calculate covariance matrices\n",
    "        s_cov = sample_covariance(est_window)\n",
    "        lw_cov = ledoit_wolf_covariance(est_window)\n",
    "\n",
    "        # Calculate GMV weights\n",
    "        w_sample = gmv_weights(s_cov, lower_bound, upper_bound)\n",
    "        w_lw = gmv_weights(lw_cov, lower_bound, upper_bound)\n",
    "\n",
    "        # Record performance\n",
    "        perf_rows.append(\n",
    "            {\n",
    "                \"date\": dates[t + 1],\n",
    "                \"sample_return\": np.dot(w_sample, next_raw_ret),\n",
    "                \"lw_return\": np.dot(w_lw, next_raw_ret),\n",
    "                \"sample_excess_return\": np.dot(w_sample, next_excess_ret),\n",
    "                \"lw_excess_return\": np.dot(w_lw, next_excess_ret),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Record weights\n",
    "        for i, tic in enumerate(tickers):\n",
    "            weight_rows.append(\n",
    "                {\n",
    "                    \"date\": dates[t],\n",
    "                    \"method\": \"sample\",\n",
    "                    \"ticker\": tic,\n",
    "                    \"weight\": w_sample[i],\n",
    "                }\n",
    "            )\n",
    "            weight_rows.append(\n",
    "                {\n",
    "                    \"date\": dates[t],\n",
    "                    \"method\": \"ledoit_wolf\",\n",
    "                    \"ticker\": tic,\n",
    "                    \"weight\": w_lw[i],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert lists to DataFrames\n",
    "    perf_df = pd.DataFrame(perf_rows).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # Add cumulative return columns\n",
    "    perf_df[\"sample_cum\"] = (1 + perf_df[\"sample_return\"]).cumprod()\n",
    "    perf_df[\"lw_cum\"] = (1 + perf_df[\"lw_return\"]).cumprod()\n",
    "\n",
    "    # Make weights_df a DataFrame\n",
    "    weights_df = pd.DataFrame(weight_rows)\n",
    "\n",
    "    return perf_df, weights_df\n",
    "\n",
    "\n",
    "def summarize_performance(perf_df):\n",
    "    \"\"\"\n",
    "    Summarize key performance metrics for each strategy.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Loop over each strategy and compute metrics\n",
    "    for label, col1, col2 in [\n",
    "        (\"sample\", \"sample_return\", \"sample_excess_return\"),\n",
    "        (\"ledoit_wolf\", \"lw_return\", \"lw_excess_return\"),\n",
    "        (\"equal_weighted\", \"ew_return\", \"ew_excess_return\"),\n",
    "        (\"value_weighted\", \"vw_return\", \"vw_excess_return\"),\n",
    "        (\"price_weighted\", \"pw_return\", \"pw_excess_return\"),\n",
    "    ]:\n",
    "        # Compute annualized return, volatility, and Sharpe ratio\n",
    "        ann_mean, ann_std = annualize_mean_std(perf_df[col1])\n",
    "        sr = sharpe_ratio(perf_df[col2])\n",
    "\n",
    "        # Append results to summary list\n",
    "        summary.append(\n",
    "            {\n",
    "                \"strategy\": label,\n",
    "                \"annual_return\": ann_mean,\n",
    "                \"annual_volatility\": ann_std,\n",
    "                \"sharpe_ratio\": sr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Convert summary list to DataFrame\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def annualize_mean_std(monthly_returns):\n",
    "    \"\"\"\n",
    "    Annualize mean and standard deviation of monthly returns.\n",
    "    \"\"\"\n",
    "    # Calculate mean and std of monthly returns\n",
    "    mean_monthly = monthly_returns.mean()\n",
    "    std_monthly = monthly_returns.std(ddof=1)\n",
    "\n",
    "    # Annualize the monthly metrics\n",
    "    ann_mean = (1 + mean_monthly) ** 12 - 1\n",
    "    ann_std = std_monthly * math.sqrt(12)\n",
    "\n",
    "    return ann_mean, ann_std\n",
    "\n",
    "\n",
    "def sharpe_ratio(monthly_excess_returns):\n",
    "    \"\"\"\n",
    "    Calculate annualized Sharpe ratio from monthly excess returns.\n",
    "    Sharpe = sqrt(12) * (mean(excess) / std(excess))\n",
    "    \"\"\"\n",
    "    sharpe = math.sqrt(12) * (\n",
    "        (monthly_excess_returns.mean())\n",
    "        / (\n",
    "            monthly_excess_returns.std(ddof=1)\n",
    "            # Handle case where std is zero\n",
    "            if monthly_excess_returns.std(ddof=1) != 0\n",
    "            else np.nan\n",
    "        )\n",
    "    )\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "def turnover_series(weight_list: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute monthly turnover series from weight history DataFrame.\n",
    "    Turnover ≈ 0.5 * Σ_i |w_t,i − w_{t−1,i}|\n",
    "    \"\"\"\n",
    "    if isinstance(weight_list, pd.DataFrame) and not weight_list.empty:\n",
    "        W = weight_list.sort_index()\n",
    "        # Compute absolute differences between consecutive rows\n",
    "        dW = W.diff().abs()\n",
    "        # Sum absolute differences across columns and scale by 0.5\n",
    "        turnover = 0.5 * dW.sum(axis=1)\n",
    "        turnover = turnover.iloc[1:]  # drop first (NaN) period\n",
    "        turnover.name = \"turnover\"\n",
    "    else:\n",
    "        turnover = pd.Series(dtype=float, name=\"turnover\")\n",
    "\n",
    "    return turnover\n",
    "\n",
    "\n",
    "def compute_turnover(weights_df):\n",
    "    \"\"\"\n",
    "    Compute the monthly turnover for each method.\n",
    "    Returns a DataFrame with columns: date, method, turnover\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    # Loop over each method\n",
    "    for method in weights_df[\"method\"].unique():\n",
    "        w = (\n",
    "            weights_df[weights_df[\"method\"] == method]\n",
    "            .pivot(index=\"date\", columns=\"ticker\", values=\"weight\")\n",
    "            .sort_index()\n",
    "        )\n",
    "        for i in range(1, len(w)):\n",
    "            prev, curr = w.iloc[i - 1].values, w.iloc[i].values\n",
    "            out.append(\n",
    "                {\n",
    "                    \"date\": w.index[i],\n",
    "                    \"method\": method,\n",
    "                    \"turnover\": float(0.5 * np.sum(np.abs(curr - prev))),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def compute_weight_stability(weights_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute the weight stability (dispersion) for each method.\n",
    "    Calculated as the standard deviation of portfolio weights at each time point.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for method in weights_df[\"method\"].unique():\n",
    "        w = (\n",
    "            weights_df[weights_df[\"method\"] == method]\n",
    "            .pivot(index=\"date\", columns=\"ticker\", values=\"weight\")\n",
    "            .sort_index()\n",
    "        )\n",
    "        for i in range(len(w)):\n",
    "            curr = w.iloc[i].values\n",
    "            out.append(\n",
    "                {\n",
    "                    \"date\": w.index[i],\n",
    "                    \"method\": method,\n",
    "                    # Standard deviation of weights as a measure of dispersion\n",
    "                    \"weight_dispersion\": float(np.std(curr)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def drawdown_series(cum_series):\n",
    "    \"\"\"\n",
    "    compute drawdown series and max drawdown\n",
    "    cum_series : pd.Series of cumulative returns (growth of $1)\n",
    "    returns    : (drawdown_series, max_drawdown_as_positive_float)\n",
    "    \"\"\"\n",
    "    # Calculate drawdowns\n",
    "    peak = cum_series.cummax()\n",
    "    dd = (cum_series / peak) - 1.0\n",
    "    # Max drawdown\n",
    "    mdd = -dd.min()\n",
    "\n",
    "    return dd, mdd\n",
    "\n",
    "\n",
    "def rolling_sharpe_series(excess_monthly_returns, window=12):\n",
    "    \"\"\"\n",
    "    rolling (annualized) sharpe using excess returns\n",
    "    sharpe = sqrt(12) * (mean(excess) / std(excess))\n",
    "    \"\"\"\n",
    "    rs = (\n",
    "        np.sqrt(12)\n",
    "        * excess_monthly_returns.rolling(window).mean()\n",
    "        / excess_monthly_returns.rolling(window).std()\n",
    "    )\n",
    "    return rs\n",
    "\n",
    "\n",
    "def sortino(excess):\n",
    "    \"\"\"\n",
    "    Sortino ratio = sqrt(12) * (mean(excess) / downside deviation)\n",
    "    \"\"\"\n",
    "    # Calculate downside deviation\n",
    "    downside = excess[excess < 0]\n",
    "    # Downside deviation is the standard deviation of negative returns\n",
    "    dd = np.sqrt((downside**2).mean())\n",
    "\n",
    "    return np.nan if dd == 0 else math.sqrt(12) * excess.mean() / dd\n",
    "\n",
    "\n",
    "def drawdown_stats(cum):\n",
    "    \"\"\"\n",
    "    compute max drawdown from cumulative returns series\n",
    "    \"\"\"\n",
    "    # Calculate drawdowns\n",
    "    peak = cum.cummax()\n",
    "    # Drawdown is the current value divided by the peak value minus 1\n",
    "    dd = cum / peak - 1\n",
    "\n",
    "    return dd.min()\n",
    "\n",
    "\n",
    "def turnover_avg(weights_df, method):\n",
    "    \"\"\"\n",
    "    Compute average turnover for a given method.\n",
    "    \"\"\"\n",
    "    w = (\n",
    "        weights_df[weights_df[\"method\"] == method]\n",
    "        .pivot(index=\"date\", columns=\"ticker\", values=\"weight\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    turns = []\n",
    "\n",
    "    for i in range(1, len(w)):\n",
    "        # Compute turnover as the sum of absolute weight changes\n",
    "        prev, curr = w.iloc[i - 1].values, w.iloc[i].values\n",
    "        turns.append(np.sum(np.abs(curr - prev)))\n",
    "\n",
    "    return np.mean(turns) if turns else np.nan\n",
    "\n",
    "\n",
    "def build_summary_plus(perf_df, weights_df):\n",
    "    \"\"\"\n",
    "    Build an extended summary DataFrame with additional metrics.\n",
    "    Returns a DataFrame with columns:\n",
    "    strategy, ann_return, ann_vol, sharpe, sortino, max_dd, calmar,\n",
    "    VaR95, CVaR95, hit_ratio, skew, kurtosis, avg_turnover, avg_weight_dispersion\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    for label, col1, col2 in [\n",
    "        (\"sample\", \"sample_return\", \"sample_excess_return\"),\n",
    "        (\"ledoit_wolf\", \"lw_return\", \"lw_excess_return\"),\n",
    "        (\"equal_weighted\", \"ew_return\", \"ew_excess_return\"),\n",
    "        (\"value_weighted\", \"vw_return\", \"vw_excess_return\"),\n",
    "        (\"price_weighted\", \"pw_return\", \"pw_excess_return\"),\n",
    "    ]:\n",
    "        # Compute annualized return and volatility\n",
    "        ann_r, ann_s = annualize_mean_std(perf_df[col1])\n",
    "        # Compute Sharpe ratio\n",
    "        sr = sharpe_ratio(perf_df[col2])\n",
    "        # Compute Sortino ratio\n",
    "        sor = sortino(perf_df[col2])\n",
    "        # Compute the cumulative returns series\n",
    "        cum = (1 + perf_df[col1]).cumprod()\n",
    "        # Compute maximum drawdown\n",
    "        mdd = drawdown_stats(cum)\n",
    "        # Compute Calmar ratio\n",
    "        calmar = ann_r / abs(mdd) if mdd < 0 else np.nan\n",
    "        # Compute 5% VaR and CVaR\n",
    "        var95 = perf_df[col1].quantile(0.05)\n",
    "        cvar95 = perf_df[col1][perf_df[col1] <= var95].mean()\n",
    "        # Compute hit ratio, skewness, and kurtosis\n",
    "        hit = (perf_df[col1] > 0).mean()\n",
    "        skew = perf_df[col1].skew()\n",
    "        kurt = perf_df[col1].kurtosis()\n",
    "\n",
    "        # Compute average turnover and weight dispersion for the method\n",
    "        turn_df = compute_turnover(weights_df[weights_df[\"method\"] == label])\n",
    "        avg_turn_over = turn_df[\"turnover\"].mean() if not turn_df.empty else np.nan\n",
    "\n",
    "        stab_df = compute_weight_stability(weights_df[weights_df[\"method\"] == label])\n",
    "        avg_weight_dispersion = (\n",
    "            stab_df[\"weight_dispersion\"].mean() if not stab_df.empty else np.nan\n",
    "        )\n",
    "\n",
    "        # Append all metrics to the summary rows\n",
    "        rows.append(\n",
    "            {\n",
    "                \"strategy\": label,\n",
    "                \"ann_return\": ann_r,\n",
    "                \"ann_vol\": ann_s,\n",
    "                \"sharpe\": sr,\n",
    "                \"sortino\": sor,\n",
    "                \"max_dd\": mdd,\n",
    "                \"calmar\": calmar,\n",
    "                \"VaR95\": var95,\n",
    "                \"CVaR95\": cvar95,\n",
    "                \"hit_ratio\": hit,\n",
    "                \"skew\": skew,\n",
    "                \"kurtosis\": kurt,\n",
    "                \"avg_turnover\": avg_turn_over,\n",
    "                \"avg_weight_dispersion\": avg_weight_dispersion,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_cumulative(perf_df):\n",
    "    \"\"\"\n",
    "    line chart: cumulative returns (growth of $1) for sample vs ledoit-wolf\n",
    "    expects perf_df with columns: date, sample_cum, lw_cum\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(perf_df[\"date\"], perf_df[\"sample_cum\"], label=\"gmv (sample)\")\n",
    "    plt.plot(perf_df[\"date\"], perf_df[\"lw_cum\"], label=\"gmv (ledoit-wolf)\")\n",
    "    plt.plot(perf_df[\"date\"], perf_df[\"ew_cum\"], label=\"equal weighted\", linestyle=\"--\")\n",
    "    plt.plot(perf_df[\"date\"], perf_df[\"vw_cum\"], label=\"value weighted\", linestyle=\"--\")\n",
    "    plt.plot(perf_df[\"date\"], perf_df[\"pw_cum\"], label=\"price weighted\", linestyle=\"--\")\n",
    "    plt.title(\"cumulative return (growth of $1)\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"cumulative\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_drawdowns(perf_df):\n",
    "    \"\"\"\n",
    "    line chart: drawdowns for sample vs ledoit-wolf\n",
    "    expects perf_df with columns: date, sample_cum, lw_cum\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    dd_s, mdd_s = drawdown_series(perf_df[\"sample_cum\"])\n",
    "    dd_l, mdd_l = drawdown_series(perf_df[\"lw_cum\"])\n",
    "    dd_ew, mdd_ew = drawdown_series(perf_df[\"ew_cum\"])\n",
    "    dd_vw, mdd_vw = drawdown_series(perf_df[\"vw_cum\"])\n",
    "    dd_pw, mdd_pw = drawdown_series(perf_df[\"pw_cum\"])\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.plot(perf_df[\"date\"], dd_s, label=f\"sample (mdd {mdd_s:.2%})\")\n",
    "    plt.plot(perf_df[\"date\"], dd_l, label=f\"ledoit-wolf (mdd {mdd_l:.2%})\")\n",
    "    plt.plot(\n",
    "        perf_df[\"date\"],\n",
    "        dd_ew,\n",
    "        label=f\"equal weighted (mdd {mdd_ew:.2%})\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        perf_df[\"date\"],\n",
    "        dd_vw,\n",
    "        label=f\"value weighted (mdd {mdd_vw:.2%})\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        perf_df[\"date\"],\n",
    "        dd_pw,\n",
    "        label=f\"price weighted (mdd {mdd_pw:.2%})\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.title(\"drawdown\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"drawdown\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rolling_sharpe(perf_df, window=12):\n",
    "    \"\"\"\n",
    "    line chart: rolling sharpe for sample vs ledoit-wolf\n",
    "    expects perf_df with columns: date, sample_return, lw_return\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    rs_s = rolling_sharpe_series(perf_df[\"sample_excess_return\"], window=window)\n",
    "    rs_lw = rolling_sharpe_series(perf_df[\"lw_excess_return\"], window=window)\n",
    "    rs_ew = rolling_sharpe_series(perf_df[\"ew_excess_return\"], window=window)\n",
    "    rs_vw = rolling_sharpe_series(perf_df[\"vw_excess_return\"], window=window)\n",
    "    rs_pw = rolling_sharpe_series(perf_df[\"pw_excess_return\"], window=window)\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.plot(perf_df[\"date\"], rs_s, label=\"sample\")\n",
    "    plt.plot(perf_df[\"date\"], rs_lw, label=\"ledoit-wolf\")\n",
    "    plt.plot(perf_df[\"date\"], rs_ew, label=\"equal weighted\")\n",
    "    plt.plot(perf_df[\"date\"], rs_vw, label=\"value weighted\")\n",
    "    plt.plot(perf_df[\"date\"], rs_pw, label=\"price weighted\")\n",
    "    plt.title(f\"rolling {window}-month sharpe\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sharpe\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_turnover_timeseries(turnover_df):\n",
    "    \"\"\"\n",
    "    line charts: turnover by method over time\n",
    "    expects turnover_df with columns: date, method, turnover\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # for method, grp in turnover_df.groupby(\"method\"):\n",
    "    #     plt.figure(figsize = (9, 3))\n",
    "    #     plt.plot(grp[\"date\"], grp[\"turnover\"], label = method)\n",
    "    #     plt.title(f\"turnover — {method}\")\n",
    "    #     plt.xlabel(\"date\")\n",
    "    #     plt.ylabel(\"0.5 * sum |Δw|\")\n",
    "    #     plt.legend()\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    df = turnover_df.sort_values(\"date\").copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    for method, grp in df.groupby(\"method\"):\n",
    "        ax.plot(grp[\"date\"], grp[\"turnover\"], label=method)\n",
    "\n",
    "    ax.set_title(\"turnover by method over time\")\n",
    "    ax.set_xlabel(\"date\")\n",
    "    ax.set_ylabel(\"0.5 * sum |Δw|\")\n",
    "    ax.legend(title=\"method\", ncol=2, loc=\"upper left\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weights_stability_timeseries(weights_stability_df):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = weights_stability_df.sort_values(\"date\").copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    for method, grp in df.groupby(\"method\"):\n",
    "        ax.plot(grp[\"date\"], grp[\"weight_dispersion\"], label=method)\n",
    "\n",
    "    ax.set_title(\"weight dispersion by method over time\")\n",
    "    ax.set_xlabel(\"date\")\n",
    "    ax.set_ylabel(\"weight dispersion (std of portfolio weights)\")\n",
    "    ax.legend(title=\"method\", ncol=2, loc=\"upper left\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_turnover_distribution(turnover_df, bins=30):\n",
    "    \"\"\"\n",
    "    histogram: turnover distribution by method\n",
    "    expects turnover_df with columns: method, turnover\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for method, grp in turnover_df.groupby(\"method\"):\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.hist(grp[\"turnover\"], bins=bins, alpha=0.85)\n",
    "        plt.title(f\"turnover distribution — {method}\")\n",
    "        plt.xlabel(\"0.5 * sum |Δw| per month\")\n",
    "        plt.ylabel(\"frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_corr_heatmap(returns_wide):\n",
    "    \"\"\"\n",
    "    heatmap: asset correlation matrix for the universe used\n",
    "    expects returns_wide wide DataFrame (assets in columns)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    corr = returns_wide.corr()\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.title(\"asset correlation (in-sample universe)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_summary_tables(summary_df, perf_df, turnover_df, tail_n=5):\n",
    "    \"\"\"\n",
    "    display key tables inline for grading\n",
    "    \"\"\"\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(summary_df)\n",
    "    display(perf_df.tail(tail_n))\n",
    "    display(turnover_df.tail(tail_n))\n",
    "\n",
    "\n",
    "def save_csvs(perf_df, weights_df, turnover_df, stability_df, summary_df):\n",
    "    \"\"\"\n",
    "    save results to csv for reporting (kept modular so you can call or skip)\n",
    "    \"\"\"\n",
    "    perf_df.to_csv(\"oos_performance.csv\", index=False)\n",
    "    weights_df.to_csv(\"weights_history.csv\", index=False)\n",
    "    turnover_df.to_csv(\"turnover.csv\", index=False)\n",
    "    stability_df.to_csv(\"stability.csv\", index=False)\n",
    "    summary_df.to_csv(\"summary_metrics.csv\", index=False)\n",
    "\n",
    "\n",
    "def make_all_charts(perf_df, turnover_df, weights_stability_df, returns_wide=None):\n",
    "    \"\"\"\n",
    "    one-click plot routine that produces all required visuals\n",
    "    \"\"\"\n",
    "    plot_cumulative(perf_df)\n",
    "    plot_drawdowns(perf_df)\n",
    "    plot_rolling_sharpe(perf_df, window=12)\n",
    "    plot_turnover_timeseries(turnover_df)\n",
    "    plot_turnover_distribution(turnover_df)\n",
    "    plot_weights_stability_timeseries(weights_stability_df)\n",
    "    if returns_wide is not None:\n",
    "        plot_corr_heatmap(returns_wide)\n",
    "\n",
    "\n",
    "def build_topn_indexes(panel: pd.DataFrame, look_back_period: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct Equal-, Value-, and Price-Weighted index levels.\n",
    "\n",
    "    Methodology:\n",
    "    - At month t, use constituents & weights formed from month t-1 information.\n",
    "    - Grow the index from t-1 to t using 'ret' (should include delistings if available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (weights_df, returns_df)\n",
    "      weights_df: columns ['EW Weights','VW Weights','PW Weights'], indexed by month t (from the 2nd month on);\n",
    "                  each cell is a pd.Series of permno->weight for that month.\n",
    "      returns_df: columns ['EW Returns','VW Returns','PW Returns'], indexed by month t (from the 2nd month on).\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Sort & prep\n",
    "    panel = panel.sort_values([\"date\", \"permno\"])\n",
    "    # Unique months (respect lookback but keep at least 2 months for t-1/t pairing)\n",
    "    months_all = pd.Index(panel[\"date\"].drop_duplicates().sort_values())\n",
    "    months = months_all[look_back_period:]\n",
    "    if len(months) < 2:\n",
    "        raise ValueError(\"Not enough months after look_back_period to compute indices.\")\n",
    "\n",
    "    # Fast month lookup\n",
    "    by_month = {d: g for d, g in panel.groupby(\"date\")}\n",
    "\n",
    "    # Levels start at 1.0 at the first available month in our slice\n",
    "    ew_level = [1.0]\n",
    "    vw_level = [1.0]\n",
    "    pw_level = [1.0]\n",
    "\n",
    "    # Per-month weights and returns (start at t = months[1], since we need t-1)\n",
    "    ew_w_list, vw_w_list, pw_w_list = [], [], []\n",
    "    ew_ret_list, vw_ret_list, pw_ret_list = [], [], []\n",
    "\n",
    "    # Loop over t (needs t-1)\n",
    "    for i in tqdm(range(1, len(months)), desc=\"Index Calculation Progress\"):\n",
    "        t_1, t = months[i - 1], months[i]\n",
    "        g_t1 = by_month.get(t_1, pd.DataFrame()).dropna(\n",
    "            subset=[\"mktcap\", \"prc\"], how=\"any\"\n",
    "        )\n",
    "        g_t = by_month.get(t, pd.DataFrame())\n",
    "\n",
    "        if g_t1.empty or g_t.empty:\n",
    "            # carry forward last level; returns = 0; weights empty\n",
    "            ew_level.append(ew_level[-1])\n",
    "            vw_level.append(vw_level[-1])\n",
    "            pw_level.append(pw_level[-1])\n",
    "\n",
    "            ew_w_list.append(pd.Series(dtype=float))\n",
    "            vw_w_list.append(pd.Series(dtype=float))\n",
    "            pw_w_list.append(pd.Series(dtype=float))\n",
    "\n",
    "            ew_ret_list.append(0.0)\n",
    "            vw_ret_list.append(0.0)\n",
    "            pw_ret_list.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Choose universe by mktcap at t-1 (Top-N if you want: .head(N))\n",
    "        chosen = g_t1.sort_values(\"mktcap\", ascending=False).set_index(\"permno\")\n",
    "\n",
    "        # Effective returns at t for chosen names (ensure alignment & numeric dtype)\n",
    "        g_t = g_t.set_index(\"permno\")\n",
    "        common = chosen.index.intersection(g_t.index)\n",
    "        if len(common) == 0:\n",
    "            ew_level.append(ew_level[-1])\n",
    "            vw_level.append(vw_level[-1])\n",
    "            pw_level.append(pw_level[-1])\n",
    "\n",
    "            ew_w_list.append(pd.Series(dtype=float))\n",
    "            vw_w_list.append(pd.Series(dtype=float))\n",
    "            pw_w_list.append(pd.Series(dtype=float))\n",
    "\n",
    "            ew_ret_list.append(0.0)\n",
    "            vw_ret_list.append(0.0)\n",
    "            pw_ret_list.append(0.0)\n",
    "            continue\n",
    "\n",
    "        r_t = pd.to_numeric(g_t.loc[common, \"ret\"], errors=\"coerce\")\n",
    "\n",
    "        # ----- Equal-Weighted -----\n",
    "        r_ew = r_t.dropna()\n",
    "        if r_ew.empty:\n",
    "            ew_gross = 1.0\n",
    "            ew_weights = pd.Series(dtype=float)\n",
    "        else:\n",
    "            ew_weights = pd.Series(1.0 / len(r_ew), index=r_ew.index)\n",
    "            ew_gross = (1.0 + r_ew).mean()\n",
    "\n",
    "        # ----- Value-Weighted (mktcap at t-1) -----\n",
    "        w_vw_raw = pd.to_numeric(chosen.loc[common, \"mktcap\"], errors=\"coerce\")\n",
    "        mask_vw = r_t.notna() & w_vw_raw.notna() & (w_vw_raw > 0)\n",
    "        r_vw = r_t.loc[mask_vw]\n",
    "        if r_vw.empty:\n",
    "            vw_gross = 1.0\n",
    "            vw_weights = pd.Series(dtype=float)\n",
    "        else:\n",
    "            w_vw = w_vw_raw.loc[mask_vw]\n",
    "            w_vw = w_vw / w_vw.sum()\n",
    "            vw_weights = w_vw.copy()\n",
    "            vw_gross = ((1.0 + r_vw) * w_vw).sum()\n",
    "\n",
    "        # ----- Price-Weighted (price at t-1) -----\n",
    "        w_pw_raw = pd.to_numeric(chosen.loc[common, \"prc\"], errors=\"coerce\").abs()\n",
    "        mask_pw = r_t.notna() & w_pw_raw.notna() & (w_pw_raw > 0)\n",
    "        r_pw = r_t.loc[mask_pw]\n",
    "        if r_pw.empty:\n",
    "            pw_gross = 1.0\n",
    "            pw_weights = pd.Series(dtype=float)\n",
    "        else:\n",
    "            w_pw = w_pw_raw.loc[mask_pw]\n",
    "            w_pw = w_pw / w_pw.sum()\n",
    "            pw_weights = w_pw.copy()\n",
    "            pw_gross = ((1.0 + r_pw) * w_pw).sum()\n",
    "\n",
    "        # Update levels\n",
    "        ew_level.append(ew_level[-1] * float(ew_gross))\n",
    "        vw_level.append(vw_level[-1] * float(vw_gross))\n",
    "        pw_level.append(pw_level[-1] * float(pw_gross))\n",
    "\n",
    "        # Store weights (at month t) and returns (gross-1)\n",
    "        ew_w_list.append(ew_weights)\n",
    "        vw_w_list.append(vw_weights)\n",
    "        pw_w_list.append(pw_weights)\n",
    "\n",
    "        ew_ret_list.append(float(ew_gross - 1.0))\n",
    "        vw_ret_list.append(float(vw_gross - 1.0))\n",
    "        pw_ret_list.append(float(pw_gross - 1.0))\n",
    "\n",
    "    # -------- Assemble outputs --------\n",
    "    idx_months = pd.to_datetime(months[:-1])  # all months incl. the first (level=1.0)\n",
    "    idx_rt = pd.to_datetime(months[1:])  # returns/weights start at second month\n",
    "\n",
    "    # Weights: store the per-month Series in an object-dtype Series\n",
    "    ew_weights_s = pd.Series(ew_w_list, index=idx_months, name=\"EW Weights\")\n",
    "    vw_weights_s = pd.Series(vw_w_list, index=idx_months, name=\"VW Weights\")\n",
    "    pw_weights_s = pd.Series(pw_w_list, index=idx_months, name=\"PW Weights\")\n",
    "    weights_df = pd.concat([ew_weights_s, vw_weights_s, pw_weights_s], axis=1)\n",
    "\n",
    "    # Returns\n",
    "    ew_returns_s = pd.Series(ew_ret_list, index=idx_rt, name=\"ew_return\")\n",
    "    vw_returns_s = pd.Series(vw_ret_list, index=idx_rt, name=\"vw_return\")\n",
    "    pw_returns_s = pd.Series(pw_ret_list, index=idx_rt, name=\"pw_return\")\n",
    "    returns_df = pd.concat([ew_returns_s, vw_returns_s, pw_returns_s], axis=1)\n",
    "\n",
    "    weights_df = weights_to_long(weights_df, panel, map_from_t_minus_1=True)\n",
    "    # # (Optional) Levels, if you want them:\n",
    "    levels_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ew_cum\": ew_level[1:],\n",
    "            \"vw_cum\": vw_level[1:],\n",
    "            \"pw_cum\": pw_level[1:],\n",
    "        },\n",
    "        index=idx_rt,\n",
    "    )\n",
    "\n",
    "    # print(levels_df.tail())\n",
    "\n",
    "    return weights_df, returns_df, levels_df  # or (levels_df, weights_df, returns_df)\n",
    "\n",
    "\n",
    "def weights_to_long(\n",
    "    weights_df: pd.DataFrame, panel: pd.DataFrame, map_from_t_minus_1: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the 'weights_df' returned by build_topn_indexes into long/tidy format:\n",
    "\n",
    "        date       method   ticker   weight\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    - weights_df index = month-end Timestamp for t (starts at months[1:])\n",
    "    - Each cell in weights_df is a pd.Series indexed by PERMNO with weight values\n",
    "    - 'panel' has at least ['date','permno','ticker'] (one row per stock per month)\n",
    "    - If tickers change over time, mapping can be taken from t-1 (default) or t\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "    # Keep only what's needed and make a month→(permno→ticker) map\n",
    "    if not {\"date\", \"permno\", \"ticker\"}.issubset(panel.columns):\n",
    "        raise ValueError(\"`panel` must contain columns: 'date', 'permno', 'ticker'.\")\n",
    "\n",
    "    p = panel[[\"date\", \"permno\", \"ticker\"]].drop_duplicates()\n",
    "    month_map = {d: g.set_index(\"permno\")[\"ticker\"] for d, g in p.groupby(\"date\")}\n",
    "\n",
    "    rows = []\n",
    "    for dt, row in weights_df.iterrows():\n",
    "        # Choose which month to use for the permno→ticker mapping\n",
    "        key = (dt - MonthEnd(1)) if map_from_t_minus_1 else dt\n",
    "        tickmap = month_map.get(key, month_map.get(dt, pd.Series(dtype=object)))\n",
    "\n",
    "        for method_name, s in row.items():\n",
    "            if s is None or len(s) == 0:\n",
    "                continue\n",
    "            # Ensure it's a Series with permno index\n",
    "            s = pd.Series(s, copy=False)\n",
    "            df = s.rename(\"weight\").reset_index().rename(columns={\"index\": \"permno\"})\n",
    "            # Map ticker; if missing, fall back to permno as string\n",
    "            df[\"ticker\"] = df[\"permno\"].map(tickmap).fillna(df[\"permno\"].astype(str))\n",
    "            df[\"date\"] = pd.to_datetime(dt)\n",
    "            # Clean method label (e.g., \"EW Weights\" -> \"ew\")\n",
    "            df[\"method\"] = (\n",
    "                str(method_name)\n",
    "                .replace(\" Weights\", \"\")\n",
    "                .replace(\"_weights\", \"\")\n",
    "                .strip()\n",
    "                .lower()\n",
    "            )\n",
    "            rows.append(df[[\"date\", \"method\", \"ticker\", \"weight\"]])\n",
    "\n",
    "    out = (\n",
    "        pd.concat(rows, ignore_index=True)\n",
    "        .sort_values([\"date\", \"method\", \"ticker\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # rename columns from ev to equal_weighted, etc\n",
    "    method_map = {\n",
    "        \"ew\": \"equal_weighted\",\n",
    "        \"vw\": \"value_weighted\",\n",
    "        \"pw\": \"price_weighted\",\n",
    "    }\n",
    "    out[\"method\"] = out[\"method\"].replace(method_map)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54389aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default ticker universe\n",
    "default_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"XOM\", \"JPM\", \"PG\", \"HD\", \"UNH\", \"V\"]\n",
    "\n",
    "# backtest parameters\n",
    "start_date = \"2015-01-01\"\n",
    "end_date   = \"2025-06-30\"\n",
    "look_back_period = 36 # int: months\n",
    "lower_bound = -0.10\n",
    "upper_bound =  0.20\n",
    "max_missing = 10\n",
    "\n",
    "shrcd_list = [10, 11]  # U.S. common stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2b4f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-01-01'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = pd.to_datetime(start_date)\n",
    "start_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49821b3",
   "metadata": {},
   "source": [
    "### Get Monthly Returns from WRDS/CRSP\n",
    "\n",
    "We query **CRSP monthly stock file (crsp.msf)** joined with `msenames` for tickers.\n",
    "The query returns monthly simple returns for each ticker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa356a70",
   "metadata": {},
   "source": [
    "### Data Coverage Validation\n",
    "\n",
    "We require each ticker to have sufficient non-missing history:\n",
    "- Within each rolling window, allow at most `max_missing` gaps.\n",
    "- Drop tickers that fail this test.\n",
    "- Then drop any remaining rows with NaNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to WRDS and fetch data\n",
    "db = wrds.Connection()\n",
    "\n",
    "# Get active permnos for the default tickers, filtering by shrcd and max missing\n",
    "permno_list, valid_tickers = get_active_permnos(db, default_tickers, start_date, max_missing, shrcd_list)\n",
    "\n",
    "# Fetch returns and risk-free rate series\n",
    "raw_returns_wide = get_returns(db, permno_list, start_date)\n",
    "end_date = raw_returns_wide.index.max()\n",
    "risk_free_rate_series = get_risk_free_rate_series(db, start_date, end_date)\n",
    "\n",
    "# Get full CRSP monthly panel to creating equal, value-, and price-weighted indexes\n",
    "df_full = get_crsp_monthly_panel(db, permno_list, start_date)\n",
    "\n",
    "# Close the WRDS connection\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873352ee",
   "metadata": {},
   "source": [
    "### Covariance Estimators\n",
    "\n",
    "- **Sample covariance**: standard historical estimate\n",
    "- **Ledoit–Wolf shrinkage**: more stable estimator (reduces noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8579d",
   "metadata": {},
   "source": [
    "### Global Minimum Variance Portfolio\n",
    "\n",
    "We solve:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\; w^T \\Sigma w\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\\sum_i w_i = 1$$  \n",
    "$$l \\leq w_i \\leq u \\;\\;\\; \\forall i$$  \n",
    "**130/30 gross exposure constraints**:  \n",
    "  $$\\sum_i w_i^+ \\leq 1.30, \\quad \\sum_i w_i^- \\leq 0.30$$  \n",
    "\n",
    "Where:\n",
    "- **w** : vector of portfolio weights  \n",
    "- **Σ** : covariance matrix of asset returns  \n",
    "- **wᵢ** : weight of asset *i*  \n",
    "- **l, u** : lower and upper bounds on each asset’s weight  \n",
    "- **wᵢ⁺** : positive part of wᵢ (long exposure)  \n",
    "- **wᵢ⁻** : negative part of wᵢ (short exposure)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build top-N indexes for the equal-, value-, and price-weighted strategies\n",
    "topn_weights_df, topn_raw_returns_df, topn_levels_df = build_topn_indexes(df_full, look_back_period)\n",
    "\n",
    "# Compute excess returns for the top-N strategies\n",
    "topn_excess_returns_df = returns_to_excess(topn_raw_returns_df, risk_free_rate_series)\n",
    "for col in topn_excess_returns_df.columns:\n",
    "    topn_excess_returns_df = topn_excess_returns_df.rename(columns={col: col.split('_')[0] + '_excess_return'})\n",
    "\n",
    "# Sort the columns alphabetically for easier comparison\n",
    "raw_returns_wide = raw_returns_wide.reindex(sorted(raw_returns_wide.columns), axis=1)\n",
    "\n",
    "# Compute excess returns for the GMV strategies\n",
    "excess_returns_wide = returns_to_excess(raw_returns_wide, risk_free_rate_series)\n",
    "\n",
    "# Backtest the minimum variance portfolios\n",
    "perf_df, weights_df = backtest_minvar(raw_returns_wide, excess_returns_wide, \\\n",
    "                                        look_back_period, lower_bound, upper_bound)\n",
    "perf_df = perf_df.set_index('date')\n",
    "\n",
    "# Combine performance DataFrames\n",
    "perf_df = pd.concat([perf_df, topn_raw_returns_df, topn_excess_returns_df, topn_levels_df], axis=1)\n",
    "perf_df = perf_df.reset_index()\n",
    "\n",
    "# Combine weights history\n",
    "weights_df = pd.concat([weights_df, topn_weights_df], axis=0)\n",
    "\n",
    "# Compute average risk-free rate over the period for annualization\n",
    "monthly_rf = risk_free_rate_series.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac9b8d",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "We compute:\n",
    "- Annualized return\n",
    "- Annualized volatility (std dev)\n",
    "- Sharpe ratio (using annual risk-free rate)\n",
    "- Portfolio turnover and weight stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summarize_performance(perf_df)\n",
    "print(\"\\nSummary statistics for Minimum Variance Portfolio:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_plus_df = build_summary_plus(perf_df, weights_df)\n",
    "print(\"\\nExtended Portflio Summary Statistics\")\n",
    "display(summary_plus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6227ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_df = compute_turnover(weights_df)\n",
    "weights_stability_df = compute_weight_stability(weights_df)\n",
    "make_all_charts(perf_df, turnover_df, weights_stability_df, raw_returns_wide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
